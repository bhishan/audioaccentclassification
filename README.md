# ğŸ—£ï¸ English Accent Classifier from Video URL

I am using **Streamlit web app** for the UI simplicity. The app can detect the **English accent** of a speaker from a **public video URL** (such as YouTube). The solution uses **transformer-based models** (like Wav2Vec2) from Hugging Face to classify accents, and supports flexible model/feature extractor replacement.

---

## ğŸš€ Features

- ğŸ¥ Accepts public video URLs: YouTube, Loom, direct `.mp4` links, etc.
- ğŸ§  Uses a pretrained Hugging Face model to classify English accents. Plug and play architecture. 
- ğŸ”Š Automatically downloads the video and extracts audio using `yt-dlp` and `ffmpeg`. I am using `imageio_ffmpeg` to avoid having to install dependency outside of the python virtual environment.
- â±ï¸ Splits audio into chunks and processes chunks of audio for classification. Chunk size can be determined as required. Meta-data driven architecture. 
- ğŸ“ˆ Returns the predicted accent, an aggregated average confidence score, highest confidence score achieved and a short summary.
- âš™ï¸ Easily replace the model/feature extractor to support other use cases.

---

## ğŸ–¥ï¸ Supported Accents (Default Model)

Using `dima806/english_accents_classification`, this app can classify into:

- ğŸ‡ºğŸ‡¸ American (US)
- ğŸ‡¬ğŸ‡§ British (England)
- ğŸ‡®ğŸ‡³ Indian
- ğŸ‡¦ğŸ‡º Australian
- ğŸ‡¨ğŸ‡¦ Canadian

You can switch to any Hugging Face-compatible model for different tasks or labels.

---

## ğŸ“¦ Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/accent-classifier.git
cd audioaccentclassification
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
streamlit run app.py
```

## Directory Structure
```bash
.
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ accent_classification.log
â”œâ”€â”€ downloads/       # temporary videos
â”œâ”€â”€ audios/          # extracted audio files
â”œâ”€â”€ processed/       # successfully handled media
â”œâ”€â”€ failed/          # errored videos/audio
```

## Design Rationale

âœ… 16 kHz Sample Rate
Wav2Vec2 and similar transformer models are pretrained on 16kHz mono audio for optimal accuracy. Therefore, I am using 16kHz in the project.

âœ… Chunk-Based Classification
Instead of processing the entire file at once, I am using sliding window technique with a window size of 10(can be changed easily) so that we have an aggregated classification as well as time-window classification to make it more robust

âœ… Pluggable Architecture
You can swap with any compatible huggingface models:

The model (Wav2Vec2ForSequenceClassification)

The extractor (Wav2Vec2FeatureExtractor)

The label map ({0: "us", 1: "england", ...})


âœ… UUID based identification
I am using uuid to distinguish individual videos processed

âœ… Structured Folder Output
Keeps things organized.

Makes debugging, automation, or retrying failed cases easier.

UUID is used for filenames to avoid collisions and allow batch-safe processing.

âœ… Logging
A single log file is used for all runs:
Info level and warning logs can be found here. This can be used for debugging purposes as well as understanding the sequence of operations. 
The log file is as follows:
logs/accent_classification.log
It records downloads, audio extraction, inference results, and errors.


## Example Run Output
![App Screenshot](screenshots/examplerun.png)

## Example Log
![App Screenshot](screenshots/examplelog.png)